{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment: Converting Your ML Experiment into a Production-Ready Project\n",
    "\n",
    "## Overview\n",
    "\n",
    "In the CMPT3830 project (Alberta Food Drive), you were each assigned to a unique machine learning challenge depending on your group. Over the semester, you assembled a comprehensive Jupyter Notebook that included exploratory data analysis (EDA), data preprocessing, feature engineering, and model development—culminating in the validation of your chosen approaches. The goal was to harness relevant data to address real-world questions, such as how donations change year over year (Group 1), which neighborhoods might yield the highest or lowest donation volumes through geospatial analysis (Group 2), or how to optimize volunteer allocation for maximum route efficiency (Group 3). Some groups tackled ward-specific performance modeling to predict route efficiency (Group 4), while others investigated property assessment values and their impact on donation volumes (Group 5), or time-to-completion and donation prediction for improved scheduling insights (Group 6). There was even a sentiment analysis component (Group 7), which used volunteer comments to classify sentiments (positive, neutral, or negative) and explore correlations with donation success.\n",
    "\n",
    "Despite the diversity in focus—be it Comparative Analysis, Geospatial Donation Prediction, Volunteer Efficiency Modeling, or Sentiment Classification—all these Jupyter Notebook experiments share a common need to be reproducible, maintainable, and ready for deployment. In other words, while the notebook stage is perfect for prototyping and rapid iteration, production projects demand best practices for code organization, versioning, testing, and environment management. By converting your existing work into a well-structured machine learning project, you ensure that others (or future versions of you!) can easily understand, reproduce, and extend the solution.\n",
    "\n",
    "This next phase of your project asks you to apply software engineering and MLOps principles—modularizing your code, automating pipelines, and setting up environments that can be deployed across different systems. In doing so, you will transform your group’s specialized focus, whether it was Year-on-Year Comparison or Sentiment Analysis of Comments, into an industrial-strength project ready for real-world usage.\n",
    "\n",
    "\n",
    "# Learning Objectives\n",
    "\n",
    "1.\t**Project Structuring**: Apply best practices to organize your project files and folders.\n",
    "2.\t**Software Engineering Principles**: Incorporate OOP patterns and well-documented, modularized code.\n",
    "3.\t**MLOps Understanding**: Practice the principles of the machine learning lifecycle, from data ingestion to model evaluation, and set up a project structure ready for continuous integration and deployment.\n",
    "4.\t**Configuration and Extensibility**: Demonstrate how to use configuration files to manage different training or preprocessing parameters.\n",
    "5.  **Notebook Organization**: Create clear, focused Jupyter notebooks for exploration, prototyping, or demonstration, ensuring each notebook covers a specific task rather than cluttering a single notebook with all experimentation.\n",
    "\n",
    "# Instructions\n",
    "\n",
    "1.\t**Identify Steps From Your Experiment**\n",
    "\n",
    "    Review your initial Jupyter Notebook thoroughly. Identify all major steps you performed, such as:\n",
    "        - Data loading\n",
    "        - Data cleaning and preprocessing\n",
    "        - Feature engineering\n",
    "        - Model training (including hyperparameter tuning)\n",
    "        - Model evaluation and performance metrics\n",
    "        - Data visualization (if relevant for your final presentation)\n",
    "        - Any custom utility functions\n",
    "\n",
    "2.\t**Map Experimental Steps to the Project Structure**\n",
    "\n",
    "    Refer to the target folder structure below. Place your code accordingly. You may not use every folder/subfolder, but strive to incorporate as many of these structural components as make sense for your project.\n",
    "\n",
    "\n",
    "    Project Structure\n",
    "\n",
    "    ```bash\n",
    "    ml_project/\n",
    "    ├── data/                # Stores datasets (raw, processed, external)\n",
    "    │   ├── raw/\n",
    "    │   ├── processed/\n",
    "    │   └── external/\n",
    "    ├── models/              # Saved or checkpointed model files (e.g., .pt, .pkl, .joblib)\n",
    "    ├── notebooks/           # Jupyter/Colab notebooks for exploration/demos\n",
    "    ├── src/\n",
    "    │   ├── train.py         # Script to train your model\n",
    "    │   ├── predict.py       # Script to make predictions\n",
    "    │   ├── preprocess.py    # Additional data preprocessing logic\n",
    "    │   ├── evaluate.py      # Model evaluation script\n",
    "    │   └── utils/           # Shared helper functions/classes\n",
    "    │       ├── model_utils.py\n",
    "    │       └── helpers.py\n",
    "    ├── configs/\n",
    "    │   ├── train_config.yaml\n",
    "    │   └── predict_config.yaml\n",
    "    ├── docs/                # Documentation (README, usage guides, etc.)\n",
    "    ├── requirements.txt\n",
    "    ├── Makefile\n",
    "    └── .gitignore           # Not required for now\n",
    "    ```\n",
    "\n",
    "    * `data/`: Include raw data, processed data, or any external data sources.\n",
    "    * `models/`: Save trained models (exported in your preferred format).\n",
    "    * `notebooks/`: Keep your original Jupyter/Colab notebooks for reference or additional demos.\n",
    "    * `src/`: Write scripts for data preprocessing, training, evaluation, and other helper functions.\n",
    "    * `configs/`: Store YAML or JSON configuration files for preprocessing parameters, training hyperparameters, or environment-specific settings.\n",
    "    * `logs/`: Capture logs or run outputs (optional at this stage, but beneficial to organize logs).\n",
    "    * `experiments/`: Keep records or notes from your experiments, including model versions or logs from training runs.\n",
    "    * `docs/`: Use for documentation (e.g., usage instructions, developer notes, architecture diagrams).\n",
    "    * `requirements.txt` or environment.yml: Document all dependencies needed to run your project.\n",
    "    * `Makefile`: Automate your setup steps (e.g., environment creation, running tests, formatting code).\n",
    "\n",
    "3. **Create a `predict.py` Module (Optional but Highly Recommended)**\n",
    "\n",
    "    Show how your trained model can be used independently by creating a module that accepts new data and outputs predictions. This can live in the `src/` folder or a subfolder like `src/prediction/`.\n",
    "\n",
    "    A simple example of `predict.py` might look like this:\n",
    "\n",
    "    ```python\n",
    "    # src/predict.py\n",
    "\n",
    "    import joblib\n",
    "    import numpy as np\n",
    "\n",
    "    class ModelPredictor:\n",
    "        def __init__(self, model_path):\n",
    "            \"\"\"\n",
    "            Initialize the predictor with a path to a trained model file.\n",
    "            :param model_path: str, path to the .pkl or .joblib model file.\n",
    "            \"\"\"\n",
    "            self.model = joblib.load(model_path)\n",
    "\n",
    "        def predict(self, input_data: np.ndarray):\n",
    "            \"\"\"\n",
    "            Make a prediction using the loaded model.\n",
    "            :param input_data: np.ndarray containing the features for prediction.\n",
    "            :return: np.ndarray of model predictions.\n",
    "            \"\"\"\n",
    "            # Input validation or preprocessing can be handled here\n",
    "            predictions = self.model.predict(input_data)\n",
    "            return predictions\n",
    "\n",
    "    if __name__ == \"__main__\":\n",
    "        # Example usage:\n",
    "        # 1. Instantiate the predictor\n",
    "        predictor = ModelPredictor(model_path=\"../models/my_trained_model.joblib\")\n",
    "\n",
    "        # 2. Create some sample input data\n",
    "        sample_input = np.array([[5.1, 3.5, 1.4, 0.2]])  # Example shape for an Iris model\n",
    "\n",
    "        # 3. Get predictions\n",
    "        preds = predictor.predict(sample_input)\n",
    "        print(\"Predictions:\", preds)\n",
    "    ```\n",
    "    **Key Points:**\n",
    "\n",
    "    * ModelPredictor is an independent software component. It does not require the rest of your project to run for predictions.\n",
    "    * You can extend this class to handle model versioning, input validation, or pre-processing steps.\n",
    "\n",
    "4. **Write Your Training and Preprocessing Scripts**\n",
    "\n",
    "    * Preprocessing (`preprocess.py`): Handle data cleaning, feature engineering, splitting into training/testing sets. Configure parameters such as missing value strategies or feature selection in configs/preprocess_config.yaml.\n",
    "    * Training (`train.py`): Train your model, implement hyperparameter tuning if needed, and save the trained model to the `models/` folder. (Optional) You refer to `configs/train_config.yaml` for parameters (e.g., learning rate, epochs, or random seed).\n",
    "    * Evaluation (`evaluate.py`): Evaluate your model’s performance using test data or hold-out datasets, and log or save metrics for later reference.\n",
    "\n",
    "5. **Utilize Object-Oriented Programming**\n",
    "    Where possible, encapsulate functionality into classes within your src/utils/helpers.py or other modules (like `preprocess.py`, `train.py` etc). For instance, you could create:\n",
    "\n",
    "    * A DataLoader class for reading datasets from data/raw/.\n",
    "    * A FeatureEngineer class for generating new features.\n",
    "    * A Trainer class for running training jobs.\n",
    "    * A Evaluator class for evaluating the trained model.\n",
    "    \n",
    "    These classes should be designed to plug and play within your scripts, so they can also be reused or extended in future projects.\n",
    "\n",
    "6. **Notebook Organization**\n",
    "    Place any Jupyter or Colab notebooks inside the `notebooks/` folder. Instead of having a single monolithic notebook, you could create multiple notebooks, each covering a specific aspect of your workflow—such as:\n",
    "\n",
    "    * Exploratory Data Analysis\n",
    "    * Feature Engineering / Data Cleaning\n",
    "    * Model Prototyping / Comparisons\n",
    "    * Visualization / Interpretations\n",
    "    \n",
    "    While this is not extrictly necessary, this approach could help you ensure that each notebook is focused and modular, making it easier for others (and your future self) to navigate and understand each step of the process. If you decide not to break your notebooks (which is fine) you are expected at least to have all your notebooks in the `notebooks/` folder with proper naming convention depending of the objective of that notebook.\n",
    "\n",
    "7. **Document Your Project**\n",
    "\n",
    "    * Add docstrings to all classes, functions, and modules.\n",
    "    * Create or update the docs/ folder with a README.md or index.md explaining how to install dependencies, run preprocessing, train the model, evaluate the model, and make predictions.\n",
    "    * Add comments describing any design decisions (e.g., “We chose this architecture because …”).\n",
    "\n",
    "8. **`requirements.txt` and `makefile`**\n",
    "    * Makefile: Include tasks for environment setup, testing, training, data acquisition etc. You can select just the enviroment setup if you don't have any automated process.\n",
    "    * requirements.txt: Include a text file with the requirements of you project.\n",
    "\n",
    "\n",
    "# Deliverables\n",
    "\n",
    "A zip file that contains all of the following aspects:\n",
    "\n",
    "1. Project Structure: A well-organized folder structure as described.\n",
    "2. Modular Code: Demonstrate OOP and modular design in your scripts.\n",
    "3. Configuration Files: YAML or JSON configs in the configs/ folder (Optinal).\n",
    "4. Documentation: README or docs describing how to run your project.\n",
    "5. Notebook Organization: Clear naming convention and organization of all your jupyter notebooks used in your CMPT3510 project\n",
    "\n",
    "\n",
    "# Final Notes\n",
    "\n",
    "This assignment sets the stage for you to practice real-world machine learning development workflows, where teamwork, versioning, containerization, and deployment matter just as much as model accuracy. By structuring your code in this manner, you make it easier to integrate with CI/CD pipelines, scale your work to more complex projects, and facilitate collaboration with other data scientists and engineers.\n",
    "\n",
    "Feel free to tailor details (e.g., additional scripts or different file naming conventions) to suit your team’s project requirements, but keep in mind the core objectives: maintain clarity, modularity, and a structure that can scale for future needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
